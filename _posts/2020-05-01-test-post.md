---
layout: post
title: "Causal inference in machine learning: tools you don’t need with Python code you don’t need either"
---

### Introduction

The purpose of this post is to show why causal inference is hard, how it fails us, and how DAGs don’t help.

Machine learning practitioners are concerned with prediction, rarely with explanation. That’s a luxury. We work even on problems for which no data generating process can possibly be written down. It is indeed unlikely for somebody to believe her LSTM model trained on William Shakespeare’s plays works because it approximates Shakespeare. Yet it works.

Recently new tools for causal inference have become available to us. The most prominent and influential ideas in this domain are directed acyclic graphs and do-calculus. There are also new tools to bring DAGs to non-experts, such as Microsoft’s Python library DoWhy. These new tools promise to help non-experts not only predict, but explain patterns in data.

People love shiny tools and there is danger in it. New tools come with the rush to adopt them, with a feeling of being in vanguard, with new opportunities. That often makes them unreliable: they are misused, substituted for a better theory, design or data.

In this post I stick with DAGs and related libraries mainly because DAGs are really popular in the machine learning community. The points I make equally apply to the Potential Outcome framework, or any other formal language to express causality.

### Why causal inference is hard, in theory

Causal inference relies on causal assumptions. Assumptions are beliefs which allow movement from statistical associations to causation.

Randomized experiments are the golden standard of causal inference because the treatment assignment is random and physically manipulated: one group gets the treatment, one does not. The assumptions here are straightforward, securable by design, and can be conveniently defended. 

When there is no control over treatment assignment, say with observational data, researchers try to model it. Modeling here is equivalent to saying “we assume that after adjusting for age, gender, social status, and smoking, runners and non-runners are so similar to each other as if they were randomly assigned to running.” Then one can regress life expectancy on running, declare that ‘running increases life expectancy by n %”, and call it a day.

The logic behind this approach is clunky. It implicitly assumes we know exactly why people start running or live long and the only piece missing is what we are trying to estimate. A story which is not very believable and a bit circular. Also, by a happy coincidence, all parts of our model have available empirical proxies measured without error. Finally, since there is no principle way to check how well the model of choice approximates the real assignment mechanism all its assumptions can be debated for eternity. 

This brings us to a situation best summarized by Jaas Sekhon: "Without an experiment, natural experiment, a discontinuity, or some other strong design, no amount of econometric or statistical modeling can make the move from correlation to causation persuasive” or, even more concisely, by Nancy Cartwright: “No Causes in, No Causes out”.

### Why causal inference is hard, in practice

The concerns raised above are better demonstrated with practical examples. Although there are plenty, I stick to three, with one from economics, epidemiology, and political science. 

In 1986 Robert Lalonde showed how econometric procedures do not replicate experimental results. He utilized an experiment where individuals were randomly selected into job programs. Randomization allowed him to estimate a programs' unbiased effect on earning. He then asked: would we have gotten the same estimate without randomization? To mimic observational data Lalonde constructed several non-experimental control groups. After comparing estimates he concluded that econometric ones fail to replicate experimental results.

Epidemiology has the same problems. Consider the story of HDL cholesterol and heart disease. It was believed that ‘good cholesterol’ protects against coronary heart disease. Researchers even declared observational studies to be robust to covariate adjustments. Yet several years later randomized experiments demonstrated that HDL doesn't protect your heart. For epidemiology this situation is not unique and many epidemiological findings are later overturned by randomized control studies.

Democracy-growth studies were a hot topic in political science back in the day. Researchers put GPD per capita or the like on the left side of their equations, democracy on the right and to avoid being unsophisticated threw a bunch of controls there: life expectancy, educational attainment, population size, foreign direct investment, among others. From 470 estimates presented in 81 papers published prior to 2006, 16% of them showed a statistically significant and negative effect of democracy on growth, 20% negative but insignificant, 38% positive but still insignificant, and 38% of the estimates, and you will be really surprised here, were positive and statistically significant. 

The pattern is clear: no matter how confident researchers are in their observational research, it does not necessarily bring them closer to the truth.

### Why DAGs don’t solve the problem, in theory

DAGs are great. They come with great representational power and nice inferential properties: given do-calculus completeness, if an effect is not identifiable with do-calculus, it’s not indefinable elsewhere, at least without additional assumptions. They are also educational: try to draw a simple instrumental variable setup to see for yourself.

But that’s not the point. The point is, the benefits DAGs offer come into play too late to rescue us from the horror causal inference is. That’s true that given a particular graph do-calculus tells us what we can estimate and what we can’t. What it does not tell us, however, is how to construct a meaningful DAG.

The whole thing is well described by George Box (1976, p 792): “Since all models are wrong the scientist must be alert to what is importantly wrong. It is inappropriate to be concerned about mice when there are tigers abroad.”

The scary tigers here are too many observable variables to reason about, god knows how many unobservable variables, things we measure with noise, things we cannot even measure. In that setting the true graph is unknown, and when the true graph is unknown the answer to whether our inference is correct is “No idea” or “No”. 

With that in mind, a lot of what DAGs proponents are saying becomes less confusing when we add ‘if the correct DAG is known’ to it: 

“The task of selecting an appropriate set of covariates to control for confounding has been reduced to a simple “roadblocks” puzzle manageable by a simple algorithm, [if the correct DAG is known]”

“Wouldn’t it be great if we could generate the same data we used for this plot from our observational data, but make it causal? With modern causal inference approaches, we can! [if the correct DAG is known]” 

### Why DAGs don’t solve the problem, in practice

DoWhy is a great library. The authors put a real effort in warning users that causal inference is hard pretty much every time the user uses the library.

Yet there are things that bother me. Consider the following quote:

“Conceptually, DoWhy was created following two guiding principles: asking causal assumptions explicit and testing robustness of the estimates to violations of those assumptions.”

The core assumption is that a chosen DAG is a correct one among the ocean of alternative DAGs - the assumption for which there are no robustness checks. It’s also the assumption which is easy to violate.

Let’s violate it then. I will use a setting described in the following tutorial - DoWhy: Different estimation methods for causal inference. There are 5 common causes, 2 instruments, one binary treatment, all other effects are bounded to be within 0.5 of the treatment effect, and the outcome is fully determined by the set of observable variables.

Although there are numerous ways to mis-specify a model, I will simulate a very simple one: there is one variable U missing. Even with one missing variable one can draw 511 different combinations of arrows. I therefore stick only with a subset of possible scenarios: U -> outcome, U -> treatment, U -> outcome and treatment, U -> treatment and a random common cause, U -> outcome and a random common cause, U -> an instrument and treatment, U -> an instrument and outcome.

```python
import os, sys

sys.path.append(os.path.abspath("../../"))

import numpy as np
import pandas as pd

import dowhy
from dowhy import CausalModel
import dowhy.datasets
from modified_linear_dataset import modified_linear_dataset
from simulation_function import simulate_dag_violations

methods = [
    ["backdoor.linear_regression", None],
    ["backdoor.propensity_score_stratification", None],
    ["backdoor.propensity_score_matching", None],
    ["backdoor.propensity_score_weighting", {"weighting_scheme": "ips_weight"}],
]

# u -> outcome
u_outcome = simulate_dag_violations(
    methods=methods,
    num_w_affected=0,
    effect_on_w=0,
    num_z_affected=0,
    effect_on_z=0,
    num_t_affected=0,
    effect_on_t=0,
    effect_on_y=10,
    times=100,
)

df_u_outcome = pd.DataFrame(u_outcome, columns=["value", "method"])
df_u_outcome["affected"] = pd.Series(
    ["outcome" for x in range(len(df_u_outcome.index))]
)

# u -> treatment
u_treatment = simulate_dag_violations(
    methods=methods,
    num_w_affected=0,
    effect_on_w=0,
    num_z_affected=0,
    effect_on_z=0,
    num_t_affected=1,
    effect_on_t=10,
    effect_on_y=0,
    times=100,
)

df_u_treatment = pd.DataFrame(u_treatment, columns=["value", "method"])
df_u_treatment["affected"] = pd.Series(
    ["treatment" for x in range(len(df_u_treatment.index))]
)

# u -> outcome and treatment
u_outcome_and_treatment = simulate_dag_violations(
    methods=methods,
    num_w_affected=0,
    effect_on_w=0,
    num_z_affected=0,
    effect_on_z=0,
    num_t_affected=1,
    effect_on_t=10,
    effect_on_y=10,
    times=100,
)

df_u_outcome_and_treatment = pd.DataFrame(
    u_outcome_and_treatment, columns=["value", "method"]
)
df_u_outcome_and_treatment["affected"] = pd.Series(
    ["outcome_and_treatment" for x in range(len(df_u_outcome_and_treatment.index))]
)

# u -> treatment and a random common cause
u_treatment_and_common_cause = simulate_dag_violations(
    methods=methods,
    num_w_affected=1,
    effect_on_w=10,
    num_z_affected=0,
    effect_on_z=0,
    num_t_affected=1,
    effect_on_t=10,
    effect_on_y=0,
    times=100,
)

df_u_treatment_and_common_cause = pd.DataFrame(
    u_treatment_and_common_cause, columns=["value", "method"]
)
df_u_treatment_and_common_cause["affected"] = pd.Series(
    [
        "treatment_and_common_cause"
        for x in range(len(df_u_treatment_and_common_cause.index))
    ]
)

# u -> outcome and a random common cause
u_outcome_and_common_cause = simulate_dag_violations(
    methods=methods,
    num_w_affected=1,
    effect_on_w=10,
    num_z_affected=0,
    effect_on_z=0,
    num_t_affected=0,
    effect_on_t=0,
    effect_on_y=10,
    times=100,
)

df_u_outcome_and_common_cause = pd.DataFrame(
    u_outcome_and_common_cause, columns=["value", "method"]
)
df_u_outcome_and_common_cause["affected"] = pd.Series(
    [
        "outcome_and_common_cause"
        for x in range(len(df_u_outcome_and_common_cause.index))
    ]
)

df_list = [
    df_u_outcome,
    df_u_treatment,
    df_u_outcome_and_treatment,
    df_u_treatment_and_common_cause,
    df_u_outcome_and_common_cause,
]

df_all = pd.concat(df_list)

df_all.to_csv("backdoor_tests.csv", sep="\t")
```

### Conclusion

Causal inference from observational data is not ‘might be’ problematic but always is. No amount of cautions and warnings can save us from misunderstanding and misuse. P-values have been abused and caused much harm, so have been statistical power, matching, instrumental variables, regression discontinuities, bar charts, so will be DAGs.

Directed acyclic graphs and do-calculus can very well be the most effective tools out there. They will not help you torture your data for causal conclusions that aren't already there.
